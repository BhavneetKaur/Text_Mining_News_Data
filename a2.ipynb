{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gurjot Singh (B00811724)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bhavneet Kaur (B00809769)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part a*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and perform part-of-speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "cats = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "news = fetch_20newsgroups(subset = 'train', categories = cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "df = pd.DataFrame(news.data, columns = ['text'])\n",
    "df['tokenized_data'] = df.apply(lambda row: nltk.word_tokenize(row[\"text\"]), axis=1)\n",
    "df['POSTags'] = nltk.pos_tag_sents(df['text'].apply(word_tokenize).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in news.data:\n",
    "    temp = set(word_tokenize(i))\n",
    "    tokens.append(temp)\n",
    "flat_list = [item for sublist in tokens for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part b*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract collocation using Frequency with filter, PMI, T-test with filter,Chi-Sq test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_freq = bigramFinder.ngram_fd.items()\n",
    "bigramFreqTable = pd.DataFrame(list(bigrams_freq), columns=['bigram','Frequency']).sort_values(by='Frequency', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "#function to filter for ADJ/NN bigrams\n",
    "def rightTypes(ngram):\n",
    "    if '@' in ngram or '<' in ngram or '>' in ngram or 'Re' in ngram or 'Subject' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in en_stopwords or word.isspace():\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "#filter bigrams\n",
    "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bigrams_pmi = bigramFinder.score_ngrams(bigrams.pmi)   \n",
    "bigramPmiTable = pd.DataFrame(list(bigrams_pmi), columns=['bigram','PMI']).sort_values(by='PMI', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_t = bigramFinder.score_ngrams(bigrams.student_t)\n",
    "bigramTtable = pd.DataFrame(list(bigrams_t), columns=['bigram','t']).sort_values(by='t', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredT_bi = bigramTtable[bigramTtable.bigram.map(lambda x: rightTypes(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_chi = bigramFinder.score_ngrams(bigrams.chi_sq)\n",
    "bigramChiTable = pd.DataFrame(list(bigrams_chi), columns=['bigram','chi-sq']).sort_values(by='chi-sq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_bi = bigramChiTable[:20].bigram.values\n",
    "t_bi = filteredT_bi[:20].bigram.values\n",
    "pmi_bi = bigramPmiTable[:20].bigram.values\n",
    "freq_bi = filtered_bi[:20].bigram.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of all the four methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency With Filter</th>\n",
       "      <th>PMI</th>\n",
       "      <th>T-test With Filter</th>\n",
       "      <th>Chi-Sq Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(writes, nothing)</td>\n",
       "      <td>('-g, SHOgraphics)</td>\n",
       "      <td>(writes, nothing)</td>\n",
       "      <td>('-g, SHOgraphics)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Lines, life)</td>\n",
       "      <td>(conforming, whilst)</td>\n",
       "      <td>(Lines, life)</td>\n",
       "      <td>(complies, 841099)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(message, Organization)</td>\n",
       "      <td>(comp.sys, alogirhtm)</td>\n",
       "      <td>(message, Organization)</td>\n",
       "      <td>(compressibility, Wheeler)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Institute, Lines)</td>\n",
       "      <td>(compacte, werkelijk)</td>\n",
       "      <td>(Institute, Lines)</td>\n",
       "      <td>(compressors, 155622)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(much, article)</td>\n",
       "      <td>(compatables, Paramus)</td>\n",
       "      <td>(much, article)</td>\n",
       "      <td>(concentrating, FREEDOM)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Lines, word)</td>\n",
       "      <td>(compataible, dungeon.lonestar.org)</td>\n",
       "      <td>(Lines, word)</td>\n",
       "      <td>(condensing, 2*)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(access.digex.net, Organization)</td>\n",
       "      <td>(complies, 841099)</td>\n",
       "      <td>(access.digex.net, Organization)</td>\n",
       "      <td>(condor, Sen.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Distribution, Organization)</td>\n",
       "      <td>(compositions, Name___________________________...</td>\n",
       "      <td>(want, Christian)</td>\n",
       "      <td>(conductivity, fouled)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(want, Christian)</td>\n",
       "      <td>(compressibility, Wheeler)</td>\n",
       "      <td>(Nntp-Posting-Host, version)</td>\n",
       "      <td>(confirms, captivity)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Nntp-Posting-Host, version)</td>\n",
       "      <td>(compressors, 155622)</td>\n",
       "      <td>(today, article)</td>\n",
       "      <td>(conforming, whilst)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(today, article)</td>\n",
       "      <td>(concentrating, FREEDOM)</td>\n",
       "      <td>(Distribution, Organization)</td>\n",
       "      <td>(congrete, docking/attaching)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(others, writes)</td>\n",
       "      <td>(condensing, 2*)</td>\n",
       "      <td>(Has, Lines)</td>\n",
       "      <td>(connexion, Assyria)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(Has, Lines)</td>\n",
       "      <td>(condor, Sen.)</td>\n",
       "      <td>(others, writes)</td>\n",
       "      <td>(conrad.appstate.edu, mackerel.gu.uwa.edu.au)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(article, Sandvik)</td>\n",
       "      <td>(conductivity, fouled)</td>\n",
       "      <td>(article, Sandvik)</td>\n",
       "      <td>(consanguineous, Frashokereti)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(fact, world)</td>\n",
       "      <td>(confirms, captivity)</td>\n",
       "      <td>(Technology, Keith)</td>\n",
       "      <td>(consecrated, liturgical)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(exist, people)</td>\n",
       "      <td>(congrete, docking/attaching)</td>\n",
       "      <td>(exist, people)</td>\n",
       "      <td>(conservation, 7e8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(writes, Jim)</td>\n",
       "      <td>(comp.lang.idl-pvwave, lpi.liant.com)</td>\n",
       "      <td>(fact, world)</td>\n",
       "      <td>(consoles, 4-byte)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(Technology, Keith)</td>\n",
       "      <td>(connexion, Assyria)</td>\n",
       "      <td>(writes, Jim)</td>\n",
       "      <td>(constrains, Geode)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(Organization, Rice)</td>\n",
       "      <td>(conrad.appstate.edu, mackerel.gu.uwa.edu.au)</td>\n",
       "      <td>(Organization, Rice)</td>\n",
       "      <td>(contempory, GORDON)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(thing, NNTP-Posting-Host)</td>\n",
       "      <td>(consanguineous, Frashokereti)</td>\n",
       "      <td>(thing, NNTP-Posting-Host)</td>\n",
       "      <td>(contributions/criticisms, *********)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Frequency With Filter  \\\n",
       "0                  (writes, nothing)   \n",
       "1                      (Lines, life)   \n",
       "2            (message, Organization)   \n",
       "3                 (Institute, Lines)   \n",
       "4                    (much, article)   \n",
       "5                      (Lines, word)   \n",
       "6   (access.digex.net, Organization)   \n",
       "7       (Distribution, Organization)   \n",
       "8                  (want, Christian)   \n",
       "9       (Nntp-Posting-Host, version)   \n",
       "10                  (today, article)   \n",
       "11                  (others, writes)   \n",
       "12                      (Has, Lines)   \n",
       "13                (article, Sandvik)   \n",
       "14                     (fact, world)   \n",
       "15                   (exist, people)   \n",
       "16                     (writes, Jim)   \n",
       "17               (Technology, Keith)   \n",
       "18              (Organization, Rice)   \n",
       "19        (thing, NNTP-Posting-Host)   \n",
       "\n",
       "                                                  PMI  \\\n",
       "0                                  ('-g, SHOgraphics)   \n",
       "1                                (conforming, whilst)   \n",
       "2                               (comp.sys, alogirhtm)   \n",
       "3                               (compacte, werkelijk)   \n",
       "4                              (compatables, Paramus)   \n",
       "5                 (compataible, dungeon.lonestar.org)   \n",
       "6                                  (complies, 841099)   \n",
       "7   (compositions, Name___________________________...   \n",
       "8                          (compressibility, Wheeler)   \n",
       "9                               (compressors, 155622)   \n",
       "10                           (concentrating, FREEDOM)   \n",
       "11                                   (condensing, 2*)   \n",
       "12                                     (condor, Sen.)   \n",
       "13                             (conductivity, fouled)   \n",
       "14                              (confirms, captivity)   \n",
       "15                      (congrete, docking/attaching)   \n",
       "16              (comp.lang.idl-pvwave, lpi.liant.com)   \n",
       "17                               (connexion, Assyria)   \n",
       "18      (conrad.appstate.edu, mackerel.gu.uwa.edu.au)   \n",
       "19                     (consanguineous, Frashokereti)   \n",
       "\n",
       "                  T-test With Filter  \\\n",
       "0                  (writes, nothing)   \n",
       "1                      (Lines, life)   \n",
       "2            (message, Organization)   \n",
       "3                 (Institute, Lines)   \n",
       "4                    (much, article)   \n",
       "5                      (Lines, word)   \n",
       "6   (access.digex.net, Organization)   \n",
       "7                  (want, Christian)   \n",
       "8       (Nntp-Posting-Host, version)   \n",
       "9                   (today, article)   \n",
       "10      (Distribution, Organization)   \n",
       "11                      (Has, Lines)   \n",
       "12                  (others, writes)   \n",
       "13                (article, Sandvik)   \n",
       "14               (Technology, Keith)   \n",
       "15                   (exist, people)   \n",
       "16                     (fact, world)   \n",
       "17                     (writes, Jim)   \n",
       "18              (Organization, Rice)   \n",
       "19        (thing, NNTP-Posting-Host)   \n",
       "\n",
       "                                      Chi-Sq Test  \n",
       "0                              ('-g, SHOgraphics)  \n",
       "1                              (complies, 841099)  \n",
       "2                      (compressibility, Wheeler)  \n",
       "3                           (compressors, 155622)  \n",
       "4                        (concentrating, FREEDOM)  \n",
       "5                                (condensing, 2*)  \n",
       "6                                  (condor, Sen.)  \n",
       "7                          (conductivity, fouled)  \n",
       "8                           (confirms, captivity)  \n",
       "9                            (conforming, whilst)  \n",
       "10                  (congrete, docking/attaching)  \n",
       "11                           (connexion, Assyria)  \n",
       "12  (conrad.appstate.edu, mackerel.gu.uwa.edu.au)  \n",
       "13                 (consanguineous, Frashokereti)  \n",
       "14                      (consecrated, liturgical)  \n",
       "15                            (conservation, 7e8)  \n",
       "16                             (consoles, 4-byte)  \n",
       "17                            (constrains, Geode)  \n",
       "18                           (contempory, GORDON)  \n",
       "19          (contributions/criticisms, *********)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigramsCompare = pd.DataFrame([freq_bi, pmi_bi, t_bi, chi_bi]).T\n",
    "bigramsCompare.columns = ['Frequency With Filter', 'PMI', 'T-test With Filter', 'Chi-Sq Test']\n",
    "bigramsCompare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part c*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is overlap between pairs of techniques, namely, \n",
    "1. Frequency with filer and T-test with filter\n",
    "2. PMI and Chi-sq Test\n",
    "\n",
    "Hence, using the union of these combination of techniques only makes sense as they have similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part a*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def clean_text(data): \n",
    "        temp = data.lower()\n",
    "        number_temp = re.sub(r'\\d+', ' ', temp)\n",
    "        result = number_temp.translate(str.maketrans(\" \",\" \", string.punctuation))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "for i in news.data:\n",
    "    temp = []\n",
    "    temp.append(clean_text(i))\n",
    "    cleaned_text.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.DataFrame(cleaned_text, columns = ['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['cleaned_data'] = df_cleaned['data'].apply(lambda x: ' '.join([word for word in x.split() if word not in (en_stopwords)])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['tokenized_data'] = df_cleaned.apply(lambda row: nltk.word_tokenize(row[\"cleaned_data\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "df_cleaned[\"stem_data\"] = df_cleaned[\"tokenized_data\"].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "df_cleaned['final_data'] = df_cleaned['stem_data'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part b*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words tf-idf weighted vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cleaned['final_data']\n",
    "y = news.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorize_data = vectorizer.fit_transform(X_train)\n",
    "vectors_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1423, 21008)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part c*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of SVM using linear scale is bit better than Multinomial NB because of the margin SVM uses. SVM uses the maximum margin approach, and hence has better boundaries for classification with higher margin. Thus, giving better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score 0.9514900794431145\n",
      "Accuracy 0.9574468085106383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectorize_data, y_train)\n",
    "out = clf.predict(vectors_test)\n",
    "print (\"F1-score\",metrics.f1_score(y_test, out, average='macro'))\n",
    "print (\"Accuracy\",metrics.accuracy_score(y_test,out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Multinomial NB\n",
      " [[134   0   0   6]\n",
      " [  0 169   2   0]\n",
      " [  0   1 178   0]\n",
      " [ 13   1   3 104]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_nb = confusion_matrix(y_test, out)\n",
    "print (\"Confusion Matrix for Multinomial NB\\n\",cm_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score 0.9596309067191687\n",
      "Accuracy 0.9639934533551555\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf_svm_k = svm.SVC(kernel = 'linear',gamma='scale')\n",
    "clf_svm_k.fit(vectorize_data, y_train)\n",
    "pred_k = clf_svm_k.predict(vectors_test)\n",
    "print (\"F1-score\",metrics.f1_score(y_test, pred_k, average='macro'))\n",
    "print (\"Accuracy\",metrics.accuracy_score(y_test,pred_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for SVM using linear scale\n",
      " [[133   1   0   6]\n",
      " [  0 171   0   0]\n",
      " [  0   2 177   0]\n",
      " [  6   5   2 108]]\n"
     ]
    }
   ],
   "source": [
    "cm_svm_k = confusion_matrix(y_test, pred_k)\n",
    "print (\"Confusion Matrix for SVM using linear scale\\n\",cm_svm_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score 0.9377630772379671\n",
      "Accuracy 0.9459901800327333\n"
     ]
    }
   ],
   "source": [
    "clf_svm = svm.SVC(gamma='scale')\n",
    "clf_svm.fit(vectorize_data, y_train)\n",
    "pred = clf_svm.predict(vectors_test)\n",
    "print (\"F1-score\",metrics.f1_score(y_test, pred, average='macro'))\n",
    "print (\"Accuracy\",metrics.accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for SVM\n",
      " [[133   1   0   6]\n",
      " [  0 171   0   0]\n",
      " [  0   2 177   0]\n",
      " [ 13   8   3  97]]\n"
     ]
    }
   ],
   "source": [
    "cm_svm = confusion_matrix(y_test, pred)\n",
    "print (\"Confusion Matrix for SVM\\n\",cm_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using non-linear kernel, the accuracy is decreased a bit because of the difference in the hyperplane created when diving the plane into different decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part d*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy obtained is almost similar as compared to the complete text even though the vocabulary formed is comparably a lot less than the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['POSTags'] = df['POSTags'].apply(lambda x: [word for (word,pos) in x if (pos == 'NN' or pos=='NNP' or pos=='NNPS' or pos=='NNS')])\n",
    "df['noun_data'] = df['POSTags'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_clean = []\n",
    "for i in df['noun_data']:\n",
    "    noun_clean.append(clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['noun_clean'] = noun_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_data'] = df['noun_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (en_stopwords)])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_data'] = df.apply(lambda row: nltk.word_tokenize(row[\"cleaned_data\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"stem_data\"] = df[\"tokenized_data\"].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "df['final_data'] = df['stem_data'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['final_data']\n",
    "y = news.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorize_data = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1423, 16583)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9512606243810668"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectorize_data, y_train)\n",
    "out = clf.predict(vectors_test)\n",
    "metrics.f1_score(y_test, out, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[137   1   0   6]\n",
      " [  0 178   2   2]\n",
      " [  0   2 182   0]\n",
      " [ 10   1   1  89]]\n"
     ]
    }
   ],
   "source": [
    "cm_nb = confusion_matrix(y_test, out)\n",
    "print (cm_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9544921416209171"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm = svm.SVC(gamma='scale')\n",
    "clf_svm.fit(vectorize_data, y_train)\n",
    "pred = clf_svm.predict(vectors_test)\n",
    "metrics.f1_score(y_test, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[137   2   2   3]\n",
      " [  0 182   0   0]\n",
      " [  0   6 178   0]\n",
      " [  6   6   0  89]]\n"
     ]
    }
   ],
   "source": [
    "cm_svm = confusion_matrix(y_test, pred)\n",
    "print (cm_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9658238579853592"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_k = svm.SVC(kernel = 'linear', gamma='scale')\n",
    "clf_svm_k.fit(vectorize_data, y_train)\n",
    "pred_k = clf_svm_k.predict(vectors_test)\n",
    "metrics.f1_score(y_test, pred_k, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[139   0   2   3]\n",
      " [  0 181   1   0]\n",
      " [  0   5 179   0]\n",
      " [  5   2   1  93]]\n"
     ]
    }
   ],
   "source": [
    "cm_svm_k = confusion_matrix(y_test, pred_k)\n",
    "print (cm_svm_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "1. https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI\n",
    "2. https://www.programcreek.com/python/example/104689/sklearn.datasets.fetch_20newsgroups\n",
    "3. https://medium.com/@nicharuch/collocations-identifying-phrases-that-act-like-individual-words-in-nlp-f58a93a2f84a\n",
    "4. https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "5. https://scikit-learn.org/stable/modules/svm.html\n",
    "6. https://stackoverflow.com/questions/33587667/extracting-all-nouns-from-a-text-file-using-nltk\n",
    "7. https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "8. https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "9. https://scikit-learn.org/0.19/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py\n",
    "10. https://scikit-learn.org/0.19/auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py\n",
    "11. https://stackoverflow.com/questions/41674573/how-to-apply-pos-tag-sents-to-pandas-dataframe-efficiently/41674780\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
